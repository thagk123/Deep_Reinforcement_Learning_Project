# DQN & Double DQN Training on LunarLander-v3
ğŸ‡¬ğŸ‡§ English

This project provides a PyTorch implementation of both DQN and Double DQN agents trained on the LunarLander-v3 environment using Gymnasium.

## Features
- Deep Q-Network (DQN) with 3 fully connected layers
- Optional Double DQN mechanism
- Experience replay buffer
- Epsilon-greedy exploration with exponential decay
- Automatic saving of the best model (based on highest episode reward)
- Agent testing with video recording of gameplay

To switch to Double DQN, uncomment the last two lines in the main() function.

## Outputs
- best_model.pth: Best performing DQN model
- test_results/: Recorded videos of evaluation episodes
- 

ğŸ‡¬ğŸ‡· Î•Î»Î»Î·Î½Î¹ÎºÎ¬

Î‘Ï…Ï„ÏŒ Ï„Î¿ project Ï…Î»Î¿Ï€Î¿Î¹ÎµÎ¯ Î¼Îµ Ï‡ÏÎ®ÏƒÎ· PyTorch Î´ÏÎ¿ Î±Î»Î³Î¿ÏÎ¯Î¸Î¼Î¿Ï…Ï‚: DQN ÎºÎ±Î¹ Double DQN Î³Î¹Î± Ï„Î¿ Ï€ÎµÏÎ¹Î²Î¬Î»Î»Î¿Î½ LunarLander-v3 Ï„Î·Ï‚ Î²Î¹Î²Î»Î¹Î¿Î¸Î®ÎºÎ·Ï‚ Gymnasium.

## Î”Ï…Î½Î±Ï„ÏŒÏ„Î·Ï„ÎµÏ‚
- ÎÎµÏ…ÏÏ‰Î½Î¹ÎºÏŒ Î´Î¯ÎºÏ„Ï…Î¿ Î¼Îµ 3 Ï€Î»Î®ÏÏ‰Ï‚ ÏƒÏ…Î½Î´ÎµÎ´ÎµÎ¼Î­Î½Î± ÎµÏ€Î¯Ï€ÎµÎ´Î±
- Î¥Ï€Î¿ÏƒÏ„Î®ÏÎ¹Î¾Î· Double DQN
- Î§ÏÎ®ÏƒÎ· ÎµÎ¼Ï€ÎµÎ¹ÏÎ¹ÎºÎ¿Ï buffer (Replay Buffer)
- Î•Î¾ÎµÏÎµÏÎ½Î·ÏƒÎ· Î¼Îµ decaying epsilon
- Î‘Ï…Ï„ÏŒÎ¼Î±Ï„Î· Î±Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· Ï„Î¿Ï… ÎºÎ±Î»ÏÏ„ÎµÏÎ¿Ï… Î¼Î¿Î½Ï„Î­Î»Î¿Ï… (Î¼Îµ Î²Î¬ÏƒÎ· Ï„Î· Î¼ÎµÎ³Î±Î»ÏÏ„ÎµÏÎ· Î±Î½Ï„Î±Î¼Î¿Î¹Î²Î®)
- Î›ÎµÎ¹Ï„Î¿Ï…ÏÎ³Î¯Î± Î´Î¿ÎºÎ¹Î¼Î®Ï‚ Î¼Îµ ÎµÎ³Î³ÏÎ±Ï†Î® Î²Î¯Î½Ï„ÎµÎ¿

Î“Î¹Î± Î½Î± ÎµÎ½ÎµÏÎ³Î¿Ï€Î¿Î¹Î®ÏƒÎµÎ¹Ï‚ Ï„Î¿Î½ Double DQN, Î±Ï€Î¿ÏƒÏ‡Î¿Î»Î¯Î±ÏƒÎµ Ï„Î¹Ï‚ Î´ÏÎ¿ Ï„ÎµÎ»ÎµÏ…Ï„Î±Î¯ÎµÏ‚ Î³ÏÎ±Î¼Î¼Î­Ï‚ Ï„Î·Ï‚ main().

## ÎˆÎ¾Î¿Î´Î¿Î¹
- best_model.pth: Î¤Î¿ ÎºÎ±Î»ÏÏ„ÎµÏÎ¿ DQN Î¼Î¿Î½Ï„Î­Î»Î¿
- test_results/: Î’Î¯Î½Ï„ÎµÎ¿ Î¼Îµ Ï„Î± test ÎµÏ€ÎµÎ¹ÏƒÏŒÎ´Î¹Î±
